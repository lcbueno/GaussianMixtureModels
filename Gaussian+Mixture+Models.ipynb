{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmxEALW3oTa6rap5lvfjxS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Institute for Machine Learning, LLC\n","## **Author:** Dr. Giancarlo Crocetti\n","## **Course:** AIM-315 Introduction to Business Analytics\n","\n","\n","**License**: This code is licensed under the Creative Common License for non-commercial use **CC BY-NC**. Refer to https://creativecommons.org/licenses/by-nc/2.0/ for more information.\n"],"metadata":{"id":"Kk2ANChbFC1e"}},{"cell_type":"markdown","source":["#Use Case: Customer Segmentation for Marketing Strategy Optimization\n","##Objective:\n","\n","The primary aim of this use case is to identify distinct segments within the bank's customer base using a Gaussian Mixture Model (GMM). By focusing on features like age, balance, and the number of contacts during the campaign, the bank aims to understand the underlying patterns in customer behavior and tailor its marketing strategies accordingly.\n","Dataset:\n","\n","The dataset is sourced from the UCI Machine Learning Repository and pertains to the direct marketing campaigns (phone calls) of a Portuguese banking institution. The campaigns aimed to promote term deposits among the bank's customers.\n","Features:\n","\n","  - **Age**: This represents the age of the clients.\n","  - **Balance**: This represents the average yearly balance (in euros) of the clients.\n","  - **Campaign**: This represents the number of contacts performed during this campaign for a client.\n","  - **Marital**: Marital Status\n","  - **Education**: Education level of customer\n","\n","##Methodology:\n","\n","  1. **Data Preprocessing**: The relevant features are selected, and any negative values in the 'balance' feature are handled to ensure data consistency.\n","  \n","  2. **Standardization**: The selected features are standardized to bring them to a common scale.\n","  \n","  3. **Modeling**: A GMM is applied to the standardized features to create customer segments. The model with the lowest AIC value across different component numbers is selected as the best model.\n","  \n","  4. **Segmentation**: The best GMM model is used to predict the segments and assign them back to the original dataset.\n","  \n","  5. ** Summary Statistics Generation**: Summary statistics for each segment are generated to understand the characteristics of the identified segments.\n","\n","##Outcome:\n","\n","The resulting segments represent distinct groups within the customer base with varying age, balance, and campaign interaction levels. These segments are then analyzed to tailor marketing strategies, including the type and frequency of contact, offer customization, and personalized communications, to enhance the effectiveness of the marketing campaigns.\n","Business Impact:\n","\n","By leveraging the insights obtained from the customer segmentation, the bank can optimize its marketing strategies to target the right customers with the right offers. This can lead to increased customer engagement, higher conversion rates, and enhanced customer satisfaction, ultimately contributing to the bank's overall growth and profitability.\n","\n","## Challenges & Considerations:\n","\n","The selected features and the number of components should be validated and may need adjustment based on the actual business context and dataset characteristics.\n","\n","The interpretation of the segments needs to align with the business understanding and domain knowledge to drive actionable insights.\n","\n","Ethical considerations and data privacy concerns need to be addressed, especially when dealing with sensitive financial data."],"metadata":{"id":"_CWWtlSk-AIH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gSW9_614O4t"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import urllib.request\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy.stats as stats\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from scipy.stats import boxcox, anderson\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.mixture import GaussianMixture\n"]},{"cell_type":"markdown","source":["# 1. Load the data"],"metadata":{"id":"FrePhb2UBcHz"}},{"cell_type":"code","source":["features = ['age', 'balance', 'campaign', 'job', 'marital', 'education', 'housing']\n","df = pd.read_csv('./bank-full.csv', sep=';', encoding='latin1', usecols=features)\n","df.head()"],"metadata":{"id":"NHEBwmMmBXZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['balance'].max()"],"metadata":{"id":"Xe3JQCMA2CoA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Standardization & Categorical Variables\n","\n","## 2.1 Standardization\n","\n","To avoid that variables have a higher influence on the model simply because they have higher values (i.e.; Monetary) we will change the unit of each variable into unit of standard deviation by applying a transformation called **standardization** which transform a variable as:\n","\n","$z = \\frac{{x - \\bar{x}}}{{s}}$\n","\n","The Standardization is performed by the `StandardScaler` while the The OneHotEncoder is a preprocessing technique used to convert categorical data variables so they can be provided to machine learning algorithms to improve predictions.\n","\n","## 2.2 Categorical Variables\n","\n","Categorical data are variables that contain label values rather than numeric values. Each label for a given attribute is mapped to a unique integer, and then each of these integers is represented in binary formatâ€”zeros and ones.\n"],"metadata":{"id":"8Q6ipZsmFkX2"}},{"cell_type":"code","source":["# FOR EXPLANATORY PURPOSES ONLY\n","# Sample DataFrame\n","df_example = pd.DataFrame({\n","    'Color': ['Red', 'Green', 'Blue', 'Green', 'Red', 'Blue', 'Blue','Yellow']\n","})\n","print(df_example)\n","print()\n","# Initializing OneHotEncoder\n","encoder = OneHotEncoder(sparse=False, drop='first')\n","\n","# Fitting and transforming the data and converting it to DataFrame\n","one_hot_encoded_arr = encoder.fit_transform(df_example[['Color']])\n","encoded_df = pd.DataFrame(one_hot_encoded_arr, columns=encoder.get_feature_names_out(['Color']))\n","\n","print(encoded_df)"],"metadata":{"id":"EW6M4DN2gP55"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Back to the GMM Analysis"],"metadata":{"id":"UANQBSjNgbpl"}},{"cell_type":"code","source":["# Selecting relevant numerical and categorical features\n","numerical_features = ['age', 'balance']\n","categorical_features = ['job', 'marital', 'education', 'housing']\n","selected_features = numerical_features + categorical_features\n","df_selected = df[selected_features]"],"metadata":{"id":"VkPpRLPmel52"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3.2 Creating the Pipeline\n","### Numerical Features\n","#### Checking for Normality\n","For numerical feature we need to check for normality using the Anderson-Darling test as we have quite a lot of data available. The Shapiro-Wilk test can be overly sensitive with larger sample size (>>100).\n","\n","In case of the variable is not normally distributed we will apply a Box-Cox transformation, by also making sure the values are positive.\n","\n","#### Positive Values\n","If the variable contains negative or zero values, we will shift the values before applying the Box-Cox transformation. Shifting will not change the shape of the distribution.\n","\n","#### Standardization\n","Finally, we will standardize the value so to have all variables in the same unit.\n","\n","### Categorical Features\n","For categorical features we will simply apply a one-hot encoding."],"metadata":{"id":"fGSfng76PHrG"}},{"cell_type":"code","source":["class BoxCoxTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self, alpha=0.05):\n","        self.alpha = alpha\n","\n","    def fit(self, X, y=None):\n","        self.lambdas_ = {}\n","        self.shifts_ = {}\n","        for col in X.columns:\n","            # Shift data if necessary\n","            min_val = X[col].min()\n","            shift = 0 if min_val > 0 else -min_val + 1\n","            self.shifts_[col] = shift\n","            shifted_data = X[col] + shift\n","\n","            # Apply Anderson-Darling test\n","            ad_test_result = anderson(shifted_data.dropna())\n","            if ad_test_result.statistic > ad_test_result.critical_values[0]:  # Comparing with the critical value at 15% significance level\n","                _, maxlog = boxcox(shifted_data.dropna())\n","                self.lambdas_[col] = maxlog\n","        return self\n","\n","    def transform(self, X):\n","        X_transformed = pd.DataFrame(index=X.index)\n","        for col in X.columns:\n","            shifted_data = X[col] + self.shifts_.get(col, 0)\n","            if col in self.lambdas_:\n","                transformed_col = boxcox(shifted_data, lmbda=self.lambdas_[col])\n","                X_transformed[col] = transformed_col\n","            else:\n","                X_transformed[col] = shifted_data\n","        return X_transformed\n","\n","# Create a pipeline for numerical features\n","numerical_pipeline = Pipeline([\n","    ('boxcox', BoxCoxTransformer()),\n","    ('scaler', StandardScaler())\n","])\n","\n","# Create a pipeline for categorical features\n","categorical_pipeline = Pipeline([\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","# Create a column transformer to combine the pipelines\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numerical_pipeline, numerical_features),\n","        ('cat', categorical_pipeline, categorical_features)\n","    ],\n","    remainder='passthrough'\n",")\n","\n","# Apply the transformations\n","transformed_data = preprocessor.fit_transform(df_selected)\n","\n","# Manually construct feature names for the one-hot encoded categorical columns\n","ohe_categories = preprocessor.named_transformers_['cat'].named_steps['onehot'].categories_\n","cat_features = [f\"{col}_{subcat}\" for col, subcats in zip(categorical_features, ohe_categories) for subcat in subcats]\n","\n","# Combining the feature names for both numerical and categorical columns\n","all_feature_names = numerical_features + cat_features\n","\n","# Convert the sparse matrix to a dense matrix\n","dense_transformed_data = transformed_data.toarray()\n","\n","# Create the final DataFrame\n","transformed_df = pd.DataFrame(data=dense_transformed_data, columns=all_feature_names, index = df.index)\n"],"metadata":{"id":"1OoYXu9tUx-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformed_df"],"metadata":{"id":"XD2ZM_g0fpKD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**NOTE:** While you can use one-hot encoded variables in a GMM, the effectiveness and interpretability of the model might be compromised.\n","\n","What we will do is to assign the segment to each observation and then look at the original data for profiling considerations."],"metadata":{"id":"ZVdS62jqY5UE"}},{"cell_type":"markdown","source":["## 2.3.2 Checking for Normality\n","A GMM model has a strong normality assumption. We will check if these fields are normally distributed and if not we will appy a box-cox transformation."],"metadata":{"id":"417c1pyiFenU"}},{"cell_type":"code","source":["columns = df_selected.shape[1]\n","\n","# Check normality assumption for numerical values (no dummies)\n","for c in df_selected[numerical_features].columns:\n","    data = df_selected[c]\n","\n","    # Visual Inspection: Histogram and Q-Q plot\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    sns.histplot(data, kde=True)\n","    plt.title(f'Histogram of {c}')\n","\n","    plt.subplot(1, 2, 2)\n","    stats.probplot(data, dist=\"norm\", plot=plt)\n","    plt.title(f'Q-Q plot of {c}')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"gII3fLMiwIhY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With the exception of `campaign`, which is really a more discrete variable, `age` and `balance` seems to have been normalized quite nicely. Well, allow some 'slack' here."],"metadata":{"id":"jNTeYvNa_X2U"}},{"cell_type":"markdown","source":["# Generating the model"],"metadata":{"id":"VPQ0m0A3HTyD"}},{"cell_type":"markdown","source":["## Estimating k - the Number of Segments"],"metadata":{"id":"pNg0p-YntB1m"}},{"cell_type":"code","source":["def find_best_gmm(X, k_range=(2, 8)):\n","    best_aic = np.inf\n","    best_bic = np.inf\n","    best_k = None\n","    best_gmm = None\n","    aic_values = []\n","    bic_values = []\n","\n","    for k in k_range:\n","        gmm = GaussianMixture(n_components=k, random_state=0).fit(X)\n","        aic = gmm.aic(X)\n","        bic = gmm.bic(X)\n","\n","        # Store the various AICs and BICs\n","        aic_values.append(aic)\n","        bic_values.append(bic)\n","\n","        if aic < best_aic:\n","            best_aic = aic\n","            best_bic = bic\n","            best_k = k\n","            best_gmm = gmm\n","\n","    return best_gmm, best_k, best_aic, best_bic, aic_values, bic_values\n","\n","# Example usage\n","# Assuming 'transformed_df' is the DataFrame from the pipeline\n","k_range = range(3,12)\n","best_gmm_model, best_k, best_aic, best_bic, aics, bics = find_best_gmm(transformed_df, k_range)\n","print(f\"Best GMM Model: {best_k} components\")\n","print(f\"AIC: {best_aic}\")\n","print(f\"BIC: {best_bic}\")"],"metadata":{"id":"HVOMJ8Y_Zycd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting AIC and BIC\n","plt.plot ([k for k in k_range], aics, label='AIC')\n","plt.plot ([k for k in k_range], bics, label='BIC')\n","plt.legend()\n","plt.xlabel('Number of Components')"],"metadata":{"id":"lgkRJaw6XZEs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There is a strong agreement between the measure, supporting a 5 components solution. While the AIC and BIC continue to decrease as we increase the components, we are looking to the simplest (less complex) model.\n","\n","If you have two or more valleys in the graph, I would usually go with the lowest number of components as I prefer explanability over precision, meaning I will peek the least complex model."],"metadata":{"id":"4BXnbifdYX8R"}},{"cell_type":"markdown","source":["# Generating Segmentation from the Best Model\n","Let's segment the original dataset using the best model."],"metadata":{"id":"oKx9LdhQDhF5"}},{"cell_type":"code","source":["# Let's build the model with 5 components\n","gmm = GaussianMixture(n_components=5, random_state=2).fit(transformed_df)\n","\n","# Assigning the segments to the original DataFrame\n","df['Segment'] = gmm.predict(transformed_df)"],"metadata":{"id":"p1RORXPfDgjN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**NOTE:** We apply the GMM model on the transformed data, but store the field `Segment` in the original dataset. This 'trick' will allow us to interpret the segments using the un-standardazed and un-transformed data."],"metadata":{"id":"yMdZ_AacEzGN"}},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"j5OwqHPUF1oa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Display the Segmentation & Generating Summaries"],"metadata":{"id":"n3yA4JM4Zde0"}},{"cell_type":"code","source":["# Set Seaborn style\n","sns.set(style=\"whitegrid\")\n","\n","# Prepare data\n","segment_counts = df['Segment'].value_counts()\n","\n","# Plot\n","fig, ax = plt.subplots()\n","ax.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%', startangle=90)\n","ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n","\n","plt.title('Customer Segmentation')\n","plt.show()"],"metadata":{"id":"wUMD9is0ZDaX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using the original dataset let's build summary statistics for both numerical and categorical variables. We will achieve this by calculating these summary statistics separaterly and then combining them into a single DataFrame."],"metadata":{"id":"I7Bhrxm6ufaO"}},{"cell_type":"code","source":["summary_stats = pd.DataFrame()\n","\n","for segment in df['Segment'].unique():\n","    segment_data = df[df['Segment'] == segment]\n","\n","    # Summary for numerical features\n","    num_summary = segment_data[numerical_features].agg(['mean', 'std', 'min', 'max', 'count']).unstack()\n","\n","    # Summary for categorical features\n","    cat_mode = segment_data[categorical_features].apply(lambda x: x.mode()[0])  # Mode\n","    cat_freq = segment_data[categorical_features].apply(lambda x: x.value_counts().values[0])  # Frequency of mode\n","\n","    cat_mode.index = [f\"{feat}_mode\" for feat in categorical_features]\n","    cat_freq.index = [f\"{feat}_freq\" for feat in categorical_features]\n","\n","    # Combining the summaries for both types of variables\n","    segment_summary = pd.concat([num_summary, cat_mode, cat_freq])\n","\n","    # Adding the segment summary to the overall summary\n","    summary_stats = pd.concat([summary_stats, segment_summary], axis=1)\n","\n","# Naming the columns after the segments\n","summary_stats.columns = [f\"Segment_{seg}\" for seg in df['Segment'].unique()]\n","\n","print(\"Summary Statistics for Each Segment:\")\n","print(summary_stats)\n"],"metadata":{"id":"l2k_eY-zT5tM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7. Describing Your Segments\n","One of the more critical steps is to elaborate and describe the segments and provide a high-level strategy on what to do next.\n","\n","Take notice of this, as this represents the most critical step; otherwise, why did we perform a segmentation analysis in the first place?\n","\n","Notice how I spent time and engaged with domain expert colleagues to write down this section of the notebook. You should do the same in a real use case.\n","\n","**NOTE:** If you do not have the necessary domain understanding (yet), feel free to invite colleagues with experience in an analytical session to describe the segments and develop a strategy to move forward. **there is nothing wrong with this and it is something that is encouraged**.\n","\n","## Segment_0: The Established Management Group\n","\n","  - **Age Profile**: Aged around 39 years on average with a broad range (20-81 years).\n","  - **Financial Profile**: Highest average balance ($1755) with a large standard deviation, indicating significant variability in financial status.\n","  - **Demographics and Preferences**: Predominantly in management roles, married, with tertiary education, and less likely to have housing loans.\n","  - **Opportunities**: Offer high-value investment products, wealth management services, and retirement planning. Upsell premium credit cards and insurance products.\n","\n","## Segment_1: The Retired Group\n","\n","  - **Age Profile**: Significantly older (average age 53), with the oldest members up to 95 years.\n","  - **Financial Profile**: High average balance ($1810), suggesting accumulated wealth.\n","  - **Demographics and Preferences**: Mostly retired, married, with secondary education, and less inclined towards housing loans.\n","  - **Opportunities**: Offer estate planning, health insurance, and senior citizen savings schemes. Promote leisure and travel-related offers.\n","\n","## Segment_2: The Mature Managers\n","\n","  - **Age Profile**: Older (around 44 years), spanning a wide age range.\n","  - **Financial Profile**: Moderate average balance ($1221) with a high standard deviation.\n","  - **Demographics and Preferences**: Management jobs, married, secondary education, with housing loans.\n","  - **Opportunities**: Retirement planning services, fixed deposits, and tax-saving investment options. Also, offer refinancing options for housing loans.\n","\n","## Segment_3: The Stable Middle-Aged Group\n","\n","  - **Age Profile**: Middle-aged group (average age around 42).\n","  - **Financial Profile**: Moderate average balance ($1258) with a substantial standard deviation.\n","  - **Demographics and Preferences**: Predominantly blue-collar, married, with secondary education, less likely to have housing loans.\n","  - **Opportunities**: Target this segment with medium-term investment products, education loans for children, and life insurance.\n","\n","## Segment_4: The Working-Class Savers\n","\n","  - **Age Profile**: Similar to Segment_0 in age, but slightly younger on average.\n","  - **Financial Profile**: Lower average balance ($1088) with a moderate standard deviation.\n","  - **Demographics and Preferences**: Mainly blue-collar workers, married, with secondary education, and more likely to have housing loans.\n","  - **Opportunities**: Focus on savings accounts with better interest rates, personal loans, and home loan products. Financial literacy programs can be beneficial."],"metadata":{"id":"wW3u6fvjuUDs"}},{"cell_type":"markdown","source":["# General Insights and Opportunities:\n","\n","  - **Cross-Selling and Up-Selling**: Tailored products based on life stage and financial capacity. Upsell premium services to high-balance segments.\n","  - **Financial Education**: Focus on segments with lower balances and less tertiary education with financial literacy programs.\n","  - **Personalized Marketing**: Use demographics to create targeted marketing campaigns for each segment.\n","  - **Digital Banking Services**: Enhance digital banking solutions for tech-savvy younger segments and simplified services for older customers.\n","  - **Customer Retention**: Offer loyalty programs and benefits to retain long-standing customers, especially in older and wealthier segments.\n","  - **Community Engagement**: Engage in community programs or sponsor events in areas with a high concentration of certain segments."],"metadata":{"id":"Sq_RkjRkjvSU"}},{"cell_type":"markdown","source":["# Expanding Your Strategy\n","Based on the detailed profiling and analysis of the customer segments, there are several strategic opportunities for expanding the bank's business. These opportunities focus on both deepening relationships with existing customers and attracting new ones:\n","\n","  - **Tailored Financial Products and Services**: Develop and offer financial products tailored to the specific needs of each segment. For example, wealth management services for the high-balance segments, affordable loan products for the working-class savers, and retirement planning for the older segments.\n","\n","  - **Enhanced Digital Banking Solutions**: Invest in digital banking technologies to appeal to younger and tech-savvy customers. Simplify the digital experience for older customers to encourage adoption. Offer online financial advisory services, mobile banking apps, and easy-to-use online platforms.\n","\n","  - **Financial Literacy and Education Programs**: Create programs targeting segments with lower average balances and educational levels. These programs can educate customers on saving strategies, investment options, and effective financial management, fostering a more financially literate customer base.\n","\n","  - **Personalized Marketing and Customer Engagement**: Implement data-driven marketing strategies to offer personalized banking experiences. Use the insights from the segmentation to create targeted marketing campaigns, loyalty programs, and personalized offers that resonate with each segment.\n","\n","  - **Community Involvement and Social Responsibility Initiatives**: Engage with the community through local events, sponsorships, and corporate social responsibility initiatives. This approach can improve brand perception and attract customers who value community involvement.\n","\n","  - **Cross-Selling and Up-Selling Strategies**: Utilize customer data to identify opportunities for cross-selling and up-selling appropriate banking products and services, such as insurance, loans, and credit cards.\n","\n","  - **Partnerships and Collaborations**: Form strategic partnerships with businesses, educational institutions, and other organizations. These partnerships can provide a channel for reaching new customers and offering exclusive deals or services.\n","\n","  - **Expansion into New Markets or Demographics**: Explore opportunities to expand into new geographic markets or target unrepresented demographics in the current customer base.\n","\n","  - **Customer Feedback and Continuous Improvement**: Regularly collect customer feedback to understand evolving needs and preferences. Use this feedback to continuously improve products, services, and customer experiences.\n","\n","  - **Diversification of Investment and Savings Options**: Offer a diverse range of investment products, catering to different risk appetites and financial goals. This could include stocks, bonds, mutual funds, and retirement savings plans.\n","\n","  - **Robust Customer Service and Support**: Strengthen customer service channels, providing quick and effective support. Offer financial advisory services to help customers make informed decisions.\n","\n","By focusing on these areas, the bank can not only improve its offerings and customer relationships but also attract new customers, thereby expanding its business and increasing its market share."],"metadata":{"id":"TBVsZKcBkLAA"}},{"cell_type":"markdown","source":["#Congratulations\n","You have carried out a complete GMM analysis in Python and learned a different way of segmenting your customers.\n","\n","I have spent a lot of time studying the segments and developing a strategy to improve the current situation. The strategy implementation should follow the strategy put forward by the business."],"metadata":{"id":"rWPxjbaqtOXv"}},{"cell_type":"code","source":[],"metadata":{"id":"GNyowkwSo2j5"},"execution_count":null,"outputs":[]}]}